{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.1.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.6 (default, Oct 26 2016 20:30:19)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--packages com.databricks:spark-csv_2.10:1.2.0 pyspark-shell'\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, spark_home + \"/python\")\n",
    "execfile(os.path.join(spark_home, 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, coalesce, array, udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_train = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/labs/lab10data/lab10_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_test = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/labs/lab10data/lab10_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_items = spark.read.format(\"csv\") \\\n",
    "                .option(\"delimiter\",\"\\t\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(\"/labs/lab10data/lab10_items.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "df_view = spark.read.format(\"csv\").load(\"/labs/lab10data/lab10_views_programmes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train.join(df_items, df_train.item_id == df_items.item_id, 'inner').drop(df_items.item_id)\n",
    "df_test = df_test.join(df_items, df_test.item_id == df_items.item_id, 'inner').drop(df_items.item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train.filter(col(\"genres\").isNotNull())\n",
    "df_test = df_test.filter(col(\"genres\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "concat_udf = F.udf(lambda cols: \",\".join([x if x is not None else \"*\" for x in cols]), StringType())\n",
    "df_train = df_train.withColumn(\"concat_featurs\",\\\n",
    "                concat_udf(array('user_id','genres')))\n",
    "df_test = df_test.withColumn(\"concat_featurs\",\\\n",
    "                concat_udf(array('user_id','genres')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "split_col = pyspark.sql.functions.split(df_train['concat_featurs'], ',')\n",
    "df_train = df_train.withColumn(\"featurs_split\", split_col)\n",
    "split_col = pyspark.sql.functions.split(df_test['concat_featurs'], ',')\n",
    "df_test = df_test.withColumn(\"featurs_split\", split_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "916157\n",
      "Драмы\n",
      "Криминал\n"
     ]
    }
   ],
   "source": [
    "for value in df_train.select(\"featurs_split\").distinct().head(1)[0][0]:\n",
    "    print value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"featurs_split\", outputCol=\"featurs_vec\")\n",
    "\n",
    "model = cv.fit(df_train)\n",
    "df_train = model.transform(df_train)\n",
    "\n",
    "model = cv.fit(df_test)\n",
    "df_test = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "df_train = df_train.withColumn('user_id', col('user_id').cast('integer'))\\\n",
    "        .withColumn('item_id', col('item_id').cast('integer'))\\\n",
    "        .withColumn('purchase', col('purchase').cast('integer'))\\\n",
    "        #.withColumn('channel_id', col('channel_id').cast('integer'))\\\n",
    "        #.withColumn('content_type', col('content_type').cast('integer'))\\\n",
    "        #.withColumn('year', col('year').cast('integer'))\\\n",
    "        #.withColumn('region_id', col('region_id').cast('integer'))\\\n",
    "        \n",
    "df_test = df_test.withColumn('user_id', col('user_id').cast('integer'))\\\n",
    "        .withColumn('item_id', col('item_id').cast('integer'))\\\n",
    "        .withColumn('purchase', col('purchase').cast('integer'))\\\n",
    "        #.withColumn('channel_id', col('channel_id').cast('integer'))\\\n",
    "        #.withColumn('content_type', col('content_type').cast('integer'))\\\n",
    "        #.withColumn('year', col('year').cast('integer'))\\\n",
    "        #.withColumn('region_id', col('region_id').cast('integer'))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train.select('user_id','item_id','purchase','featurs_vec')#,'channel_id','content_type','year','region_id')\n",
    "df_test = df_test.select('user_id','item_id','featurs_vec')#,'channel_id','content_type','year','region_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train.withColumn('purchase', col('purchase').cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+--------------------+\n",
      "|user_id|item_id|purchase|         featurs_vec|\n",
      "+-------+-------+--------+--------------------+\n",
      "| 885512| 100140|       0|(2024,[2,1010],[1...|\n",
      "| 885581| 100140|       0|(2024,[2,1519],[1...|\n",
      "| 885864| 100140|       0|(2024,[2,1983],[1...|\n",
      "| 886006| 100140|       0|(2024,[2,1689],[1...|\n",
      "| 886038| 100140|       0|(2024,[2,1766],[1...|\n",
      "+-------+-------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      " |-- featurs_vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['user_id','item_id','genres_vec'],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(df_train)\n",
    "df_train = output.select(\"features\",\"label\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "output = assembler.transform(df_test)\n",
    "df_test = output.selectExpr(\"user_id\",\"item_id\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: string, item_id: string, purchase: int, featurs_vec: vector]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = df_train.randomSplit([0.8, 0.2], seed=12345)\n",
    "train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(2024, {4: 0.0119, 7: 0.0205, 10: 0.0006, 14: 0.0001, 16: 0.0008, 19: 0.0986, 24: 0.0, 25: 0.0093, 26: 0.0022, 30: 0.014, 37: 0.0004, 38: 0.0002, 42: 0.0005, 54: 0.0006, 86: 0.0305, 111: 0.0707, 239: 0.0032, 621: 0.0096, 718: 0.0676, 782: 0.023, 818: 0.0793, 866: 0.0698, 910: 0.0102, 1300: 0.0011, 1762: 0.4137, 1813: 0.0156, 1974: 0.0459, 2017: 0.0})"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "rf = RandomForestClassifier(numTrees=3, maxDepth=4, labelCol=\"purchase\", featuresCol=\"featurs_vec\", seed=42)\n",
    "model = rf.fit(train)\n",
    "model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "predictions.select(\"prediction\", \"purchase\")\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"purchase\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(prediction=0.0)]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.select(\"prediction\").distinct().collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "numFolds = 5\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [2, 4, 6])\n",
    "             .addGrid(rf.maxBins, [20, 60])\n",
    "             .build())\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"purchase\", featuresCol=\"featurs_vec\", numTrees=20)\n",
    "evaluator = BinaryClassificationEvaluator()  \n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=numFolds)\n",
    "\n",
    "model = crossval.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "split_udf = udf(lambda value: value[1].item(), FloatType())\n",
    "result = model.transform(df_test).withColumn('purchase', split_udf('probability'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result = result.sort(result.user_id,result.item_id)\n",
    "result = result.select('user_id', 'item_id','purchase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------+\n",
      "|user_id|item_id|    purchase|\n",
      "+-------+-------+------------+\n",
      "|   1654|    336|0.0021516618|\n",
      "|   1654|    678|0.0021516618|\n",
      "|   1654|    691|0.0021516618|\n",
      "|   1654|    696|0.0021516618|\n",
      "|   1654|    763|0.0021516618|\n",
      "+-------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"lab10.csv\")\n",
    "!hadoop fs -getmerge lab10.csv ~/lab10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id,item_id,purchase\r\n",
      "1654,336,0.0021516618\r\n",
      "1654,678,0.0021516618\r\n",
      "1654,691,0.0021516618\r\n",
      "1654,696,0.0021516618\r\n",
      "1654,763,0.0021516618\r\n",
      "1654,795,0.0021516618\r\n",
      "1654,861,0.0021516618\r\n",
      "1654,1137,0.0021516618\r\n",
      "1654,1159,0.0021516618\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "cat ~/lab10.csv | head -n10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00215166,  0.00193721,  0.00168565,  0.0016492 ,  0.0019519 ,\n",
       "        0.00153423,  0.00148588,  0.00312257,  0.00332233,  0.00944003,\n",
       "        0.00106822,  0.00512769])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('~/lab10.csv').purchase.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cp /tmp/uaa/lab10.csv ~/lab10.csv_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mBankScoring\u001b[0m/          lab07s.json            part-00000\r\n",
      "\u001b[01;34mClassRecomend\u001b[0m/        lab08.json             project01_gender-age.csv\r\n",
      "hadoop-streaming.jar  lab08s.json            \u001b[01;34mRaiting\u001b[0m/\r\n",
      "\u001b[01;34mHiveUserProfile\u001b[0m/      lab09.csv              \u001b[01;34mRecSysMovieALS\u001b[0m/\r\n",
      "lab03s_domains.txt    lab09s.csv             \u001b[01;34mRecSysMovies\u001b[0m/\r\n",
      "lab03_users.txt       lab10.csv              \u001b[01;34mRecSysProj\u001b[0m/\r\n",
      "lab04.csv             lab10.csv_1            \u001b[01;34mRecSysPurchesFilm\u001b[0m/\r\n",
      "lab04s.csv            Lesson_1.ipynb         \u001b[01;34mSpark\u001b[0m/\r\n",
      "lab05.json            \u001b[01;34mMapReducePredAge\u001b[0m/      stdout\r\n",
      "lab06.json            \u001b[01;34mMapReduceTop350\u001b[0m/       \u001b[01;34mTwoTextCompare_Sentiment\u001b[0m/\r\n",
      "lab06s.json           \u001b[01;34mMapReduceWordCount\u001b[0m/\r\n",
      "lab07.json            \u001b[01;34mMapReduseLoadToHBase\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id,item_id,purchase\r\n",
      "1654,336,0.0102475\r\n",
      "1654,678,0.00931158\r\n",
      "1654,691,0.00962465\r\n",
      "1654,696,0.00986544\r\n",
      "1654,763,0.0109576\r\n",
      "1654,795,0.0144923\r\n",
      "1654,861,0.0107935\r\n",
      "1654,1137,0.011556\r\n",
      "1654,1159,0.00992513\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "cat ~/lab10.csv_1 | head -n10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cp ~/lab10.csv_1 ~/lab10s.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id,item_id,purchase\r\n",
      "1654,336,0.0021516618\r\n",
      "1654,678,0.0021516618\r\n",
      "1654,691,0.0021516618\r\n",
      "1654,696,0.0021516618\r\n",
      "1654,763,0.0021516618\r\n",
      "1654,795,0.0021516618\r\n",
      "1654,861,0.0021516618\r\n",
      "1654,1137,0.0021516618\r\n",
      "1654,1159,0.0021516618\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "cat ~/lab10.csv | head -n10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
